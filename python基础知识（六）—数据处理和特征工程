import numpy as np
import pandas as pd
iris=pd.read_excel(r'D:\222学习\数据分析案例资料\鸢尾花数据.xlsx',header=None,names=['V1','V2','V3','V4','V5','V6'])
iris.head()
iris2=iris.copy()

######################################
###（1）筛选数据集
#（1.1）筛选V2列数据为df1
#方法一：
df1=iris.V2
df1.head()
#方法二：
df1=iris['V2']
df1.head()

#(1.2)筛选出花种为setosa的观测数据df2
df2=iris.loc[iris.V5=='setosa',:]
df2.head()

#(1.3)筛选出花种为setosa，且V1大于5的数据df3
df3=iris.loc[(iris.V1>5)&(iris.V5=='setosa'),:]
df3.head()

#(1.4)筛选出花种为setosa，且V1大于5的V1和V5列数据为df4
df4=iris.loc[(iris.V1>5)&(iris.V5=='setosa'),['V1','V5']]
df4.head()

#(1.5)筛选出前5行的第1和第5列数据为df5
df5=iris.iloc[:5,[1,5]]
df5

'''小心iloc的语法'''
####################################
###（2）删除变量,更改变量名
#（2.1）删除iris2的V1和V5变量
iris2.drop(['V1','V5'],axis=1).head()

'''奇怪这里的axis=0表示行，1表示列
数据集iris2本身没有变化
要想改变数据集iris2，需要设置inplace=True'''

#(2.2)将iris2的变量V1改为C1，V2改为C2
iris2.rename(columns={'V1':'C1','V2':'C2'},inplace=True)
iris2.head()

##############################################
###（3）数据类型转换和排序
#（3.1）将iris2的C1和C2转化为浮点类型
iris3=iris2.astype({'C1':'str','C2':'str'})
iris3.dtypes

iris3=iris2.astype({'C1':'float','C2':'float'})
iris3.dtypes

#(3.2)iris按照V1的数据降序排序,V2升序排序
iris.sort_values(by=['V1','V2'],ascending=[False,True]).head()

################################################
###(4)去重
#（4.1）检查数据集iris是否有重复观测值
iris.duplicated().head()

#(4.2)计算重复观测的行数
print(sum(iris.duplicated()))

#(4.3)删除重复观测的行
iris3.drop_duplicates().head()

#(4.4)iris3中V5列的重复
iris3.duplicated(subset='V5')

##################################################
###（5）抽样
#（5.1）从iris中抽出80%的数据为训练数据，其余为测试数据
train=iris.sample(frac=0.8,random_state=1)
test=iris.loc[~iris.index.isin(train.index),:]
print(train.shape,'\n',test.shape)

###################################################
###(6)频数统计
#（6.1）统计iris数据集中V5的各水平的频次
iris.V5.value_counts()

#(6.2)求iris数据集中V5各水平的百分比
iris.V5.value_counts()/sum(iris.V5.value_counts())

#(6.3)将iris中V5和V6做交叉统计
pd.crosstab(index=iris.V5,columns=iris.V6)

######################################################
###(7)缺失值处理
df=pd.DataFrame([[1,2,3],[np.nan,6,7],[np.nan,np.nan,np.nan],[10,np.nan,np.nan]],columns=['V1','V2','V3'])
print(df,'\n')

#(7.1)总览数据集df是否存在缺失值
any(df.isnull())

#(7.2)df每一列是否有缺失值，及缺失比例
#方法一：
is_null=[]
null_ratio=[]
for col in df.columns:
    is_null.append(any(pd.isnull(df[col])))
    null_ratio.append(float(round(sum(pd.isnull(df[col]))/df.shape[0],2)))
print(is_null,'\n',null_ratio,'\n')

#方法二：
is_null=lambda x:any(pd.isnull(x))
df.apply(func=is_null,axis=0)

#(7.3)每一行是否有缺失值
#方法一：
is_null=[]
for index in list(df.index):
    is_null.append(any(pd.isnull(df.iloc[index,:])))
print(is_null,'\n')

#方法二：
is_null=lambda x:any(pd.isnull(x))
df.apply(func=is_null,axis=1)

#(7.4)删除含有缺失值的观测数据
df1=df.copy()
df1.dropna()

#(7.5)删除观测值全部缺失的行
df2=df.copy()
df2.dropna(how='all')

#(7.6)将缺失值向前替补
df3=df.copy()
df3.fillna(method='ffill')

#(7.7)将缺失值向后替补
df4=df.copy()
df4.fillna(method='bfill')

#(7.8)不同列分别用均值，中位数，最大值替补
df5=df.copy()
df5.fillna(value={'V1':df.V1.mean(),
           'V2':df.V2.median(),
           'V3':df.V3.max()})

######################################3
###（8）映射函数在数据处理的使用
score=pd.DataFrame({'yw':[70,80,90],'sx':[100,120,80],'yy':[110,90,85],'是否':['no','yes','yes'],'gender':['男','女','男']},columns=['yw','sx','yy','是否','gender'])

#(8.1)每个学生的平均成绩
score['jz']=score.iloc[:,0:3].apply(func=np.mean,axis=1)
print(score.head())

#(8.2)每门学科的平均成绩
score.iloc[:,0:3].apply(func=np.mean,axis=0)

#####################################################
###(9)分组汇总
#(9.1)在数据集score中统计男女同学的各科平均分
groupby1=score.groupby(['gender'])
groupby1.aggregate(np.mean)

#（9.2）在数据集score中统计性别为一级分类，是否为二级分类的各科平均分
groupby2=score.groupby(['gender','是否'])
groupby2.aggregate(np.mean)

#（9.3）在数据集score中统计性别为一级分类，是否为二级分类的语文的均值，数学的中位数
groupby3=score.groupby(['gender','是否'])
groupby3.aggregate({'yw':np.mean,'sx':np.median})

##########################################################
###(10)离散变量的哑变量处理
#（10.1）将数据集score中的是否、gender列变为0，1处理,删除多余变量
s1=pd.get_dummies(score,columns=['是否','gender'])
s1.drop(['是否_yes','gender_女'],axis=1).head()

###########################################################
###(11)分段处理
#（11.1）将iris数据集中V1分为0-1.0为A，1.0-2.0为B，2.0-3.0为C，3.0以上为D
iris['cut']=pd.cut(iris.V1,bins=[0,1.0,2.0,3.0,100],right=False,
                  labels=['A','B','C','D'])
iris.head()
'''right参数为FALSE表示区间不含上限'''

#############################################################
###（12）数据集合并
#（12.1）数据集纵向合并，将D:\222学习\2223中的数据纵向合并
import os
import pandas as pd
path=r'D:\222学习\2223\\'
fname=os.listdir(path)

df=[]
for f in fname:
    df.append(pd.read_excel(path+f))
    alldata=pd.concat(df,ignore_index=True)
alldata

#(12.2)数据集的横向扩展
#语法：pd.merge(表1，表2，how='inner'或'left'或'right'，on=公共字段(或left_on=,right_on=）
